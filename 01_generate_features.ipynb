{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_generate_features.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabyasm/manu_data_competition/blob/master/01_generate_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xowUx222HMDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download & Install pytorch GPU version"
      ]
    },
    {
      "metadata": {
        "id": "eXLco-NbXDiF",
        "colab_type": "code",
        "outputId": "49d33943-e21d-427d-8f8b-71314b2c4d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58a34000 @  0x7f51c9d742a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsG0OtFW3gQv",
        "colab_type": "code",
        "outputId": "80d71579-2101-4766-c0ca-6383ce8fe49b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy.random import RandomState\n",
        "import pickle\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.cross_validation import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score as f1\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "EolKlbnADZFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download RAW test and train files (I've loaded them in aws )"
      ]
    },
    {
      "metadata": {
        "id": "gc9xN6napswv",
        "colab_type": "code",
        "outputId": "becdf738-00ad-4fc9-96f1-0c6d173054d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!curl -Lo dataset/Competition_Train_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Train_Data.txt\n",
        "!curl -Lo dataset/Competition_Test_Data.txt https://s3-us-west-2.amazonaws.com/manu00/Competition_Test_Data.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1102k  100 1102k    0     0   670k      0  0:00:01  0:00:01 --:--:--  670k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  274k  100  274k    0     0   278k      0 --:--:-- --:--:-- --:--:--  278k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Dy09WJIcDvsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Infersent with GloVe Embedding (let's call it v1)"
      ]
    },
    {
      "metadata": {
        "id": "eWGADKc5cTXG",
        "colab_type": "code",
        "outputId": "03e616d5-81c9-4d3d-cc4c-74e523f7b774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -Lo models.py https://raw.githubusercontent.com/facebookresearch/InferSent/master/models.py\n",
        "!mkdir encoder\n",
        "!curl -Lo encoder/infersent1.pkl https://s3.amazonaws.com/senteval/infersent/infersent1.pkl\n",
        "!curl -Lo encoder/infersent2.pkl https://s3.amazonaws.com/senteval/infersent/infersent2.pkl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 29737  100 29737    0     0  85697      0 --:--:-- --:--:-- --:--:-- 85697\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  10.9M      0  0:00:13  0:00:13 --:--:-- 12.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  13.0M      0  0:00:11  0:00:11 --:--:-- 16.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mog-vBalEFLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2GB GloVe embedding Incoming"
      ]
    },
    {
      "metadata": {
        "id": "-rWcJKekYt7W",
        "colab_type": "code",
        "outputId": "3762bdd9-98ff-4d04-a54d-b16de94bbdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mkdir dataset/GloVe\n",
        "!curl -Lo dataset/GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip dataset/GloVe/glove.840B.300d.zip -d dataset/GloVe/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  5915k      0  0:05:59  0:05:59 --:--:-- 5990k\n",
            "Archive:  dataset/GloVe/glove.840B.300d.zip\n",
            "  inflating: dataset/GloVe/glove.840B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uluXRXwqYOel",
        "colab_type": "code",
        "outputId": "476fde77-6fac-4f9c-c971-360b06eef206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from models import InferSent\n",
        "V = 1\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent = InferSent(params_model)\n",
        "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "# Keep it on CPU or put it on GPU\n",
        "use_cuda = True\n",
        "infersent = infersent.cuda() if use_cuda else infersent\n",
        "\n",
        "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
        "model_version = V\n",
        "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else 'dataset/fastText/crawl-300d-2M.vec'\n",
        "infersent.set_w2v_path(W2V_PATH)\n",
        "infersent.build_vocab_k_words(K=500000) #load 50k most frequent word"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I9ZVd3bJ8ZDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ** FastText Common Crawl - 1.5G Embedding - let's call Infersent V2**"
      ]
    },
    {
      "metadata": {
        "id": "9GTPRSvu8dy4",
        "colab_type": "code",
        "outputId": "601827a8-970a-4676-f7b8-adda51f22f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mkdir dataset/fastText\n",
        "!curl -Lo dataset/fastText/crawl-300d-2M.vec.zip https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
        "!unzip dataset/fastText/crawl-300d-2M.vec.zip -d dataset/fastText/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1453M  100 1453M    0     0  16.3M      0  0:01:28  0:01:28 --:--:-- 21.7M\n",
            "Archive:  dataset/fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: dataset/fastText/crawl-300d-2M.vec  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1xBxc38y9uEF",
        "colab_type": "code",
        "outputId": "0eb368f4-2439-42e3-a12a-6e64e83b3b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from models import InferSent\n",
        "V = 2\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent2 = InferSent(params_model)\n",
        "infersent2.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "# Keep it on CPU or put it on GPU\n",
        "use_cuda = True\n",
        "infersent2 = infersent2.cuda() if use_cuda else infersent2\n",
        "\n",
        "# If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n",
        "model_version = V\n",
        "W2V_PATH = 'dataset/GloVe/glove.840B.300d.txt' if model_version == 1 else 'dataset/fastText/crawl-300d-2M.vec'\n",
        "infersent2.set_w2v_path(W2V_PATH)\n",
        "infersent2.build_vocab_k_words(K=500000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzBD-q--E-sZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Let's try Google's Universal Sentense Encoder (Transfer) - [link](https://tfhub.dev/google/universal-sentence-encoder-large/3)**"
      ]
    },
    {
      "metadata": {
        "id": "l3_BX0x4tROv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **USE V3 model load**"
      ]
    },
    {
      "metadata": {
        "id": "nITkwa9Fta5l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BChezKzBtxjT",
        "colab_type": "code",
        "outputId": "e6d3cee7-1539-424b-c666-02f086e2f0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kri3LFvBX5xf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Download common crawl vector in Spacy specific format (1.5G - again)**"
      ]
    },
    {
      "metadata": {
        "id": "uBZNWLEcXfE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "f539be2d-2e3b-4441-92ec-60efb6f312c4"
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
        "!python -m spacy init-model en /tmp/fasttext-vectors --vectors-loc crawl-300d-2M.vec.zip"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-06 18:51:59--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.20.97\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.20.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  21.9MB/s    in 71s     \n",
            "\n",
            "2018-11-06 18:53:11 (20.5 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n",
            "Reading vectors from crawl-300d-2M.vec.zip\n",
            "Open loc\n",
            "tcmalloc: large alloc 2400002048 bytes == 0x33ee000 @  0x7f6a9a5d5001 0x7f6a98349b85 0x7f6a983acb43 0x7f6a983aea86 0x7f6a98446868 0x511b75 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x586ddd 0x59cf9e 0x4f6903 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x4ffa2a 0x5117df 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338\n",
            "1999995it [03:41, 9034.14it/s]\n",
            "Creating model...\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "\u001b[93m    Sucessfully compiled vocab\u001b[0m\n",
            "    1999715 entries, 1999995 vectors\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zNwPEqF_ZjPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a3d69d37-b60a-49cd-f3eb-7f5546a95c71"
      },
      "cell_type": "code",
      "source": [
        "#Code for obtaining sentence vector from spacy module\n",
        "!python -m spacy download en\n",
        "import spacy\n",
        "nlp_ft = spacy.load('en')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6qFv7VYBaNPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_spacy_cc_similarity():\n",
        "  i = 0\n",
        "  trainfts = []\n",
        "  testfts =[]\n",
        "  while i < len(traintext[0]):\n",
        "    doc1 = nlp_ft(traintext[0][i])\n",
        "    doc2 = nlp_ft(traintext[1][i])\n",
        "    trainfts.append(doc2.similarity(doc1))\n",
        "    i = i+1\n",
        "  print(\"done\")\n",
        "  i = 0\n",
        "  while i < len(testtext[0]):\n",
        "    doc1 = nlp_ft(testtext[0][i])\n",
        "    doc2 = nlp_ft(testtext[1][i])\n",
        "    testfts.append(doc2.similarity(doc1))\n",
        "    i = i+1\n",
        "    #print(\" \"+str(i))\n",
        "  \n",
        "  return trainfts, testfts\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MGHNTxVt0Lp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Basic text features like count of common word etc. **"
      ]
    },
    {
      "metadata": {
        "id": "kq-nmVFJhTJj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uODJSdOmiBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feats(A, B):\n",
        "    \"\"\"\n",
        "    Compute additional features (similar to Socher et al.)\n",
        "    These alone should give the same result from their paper (~73.2 Acc)\n",
        "    \"\"\"\n",
        "    tA = [t.split() for t in A]\n",
        "    tB = [t.split() for t in B]\n",
        "    \n",
        "    nA = [[w for w in t if is_number(w)] for t in tA]\n",
        "    nB = [[w for w in t if is_number(w)] for t in tB]\n",
        "\n",
        "    features = np.zeros((len(A), 6))\n",
        "\n",
        "    # n1\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) == set(nB[i]):\n",
        "            features[i,0] = 1.\n",
        "\n",
        "    # n2\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) == set(nB[i]) and len(nA[i]) > 0:\n",
        "            features[i,1] = 1.\n",
        "\n",
        "    # n3\n",
        "    for i in range(len(A)):\n",
        "        if set(nA[i]) <= set(nB[i]) or set(nB[i]) <= set(nA[i]): \n",
        "            features[i,2] = 1.\n",
        "\n",
        "    # n4\n",
        "    for i in range(len(A)):\n",
        "        features[i,3] = 1.0 * len(set(tA[i]) & set(tB[i])) / len(set(tA[i]))\n",
        "\n",
        "    # n5\n",
        "    for i in range(len(A)):\n",
        "        features[i,4] = 1.0 * len(set(tA[i]) & set(tB[i])) / len(set(tB[i]))\n",
        "\n",
        "    # n6\n",
        "    for i in range(len(A)):\n",
        "        features[i,5] = 0.5 * ((1.0*len(tA[i]) / len(tB[i])) + (1.0*len(tB[i]) / len(tA[i])))\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HbBQO85ikBMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_sentense_fts(modelname, loc='./dataset/'):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    print ('Preparing data...')\n",
        "    traintext, testtext, labels, uid = load_data(loc)\n",
        "\n",
        "    # Reduce logging output.\n",
        "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "    train_features = []\n",
        "    test_features = []\n",
        "    \n",
        "    if modelname == \"infersent\":\n",
        "        print ('Computing ' + modelname + 'embedding for train data...')\n",
        "\n",
        "        trainA = infersent.encode(traintext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        trainB = infersent.encode(traintext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "        print ('Computing ' + modelname + 'embedding for test data...')\n",
        "        \n",
        "        testA = infersent.encode(testtext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        testB = infersent.encode(testtext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "    if modelname == \"infersent2\":\n",
        "        print ('Computing ' + modelname + 'embedding for train data...')\n",
        "\n",
        "        trainA = infersent2.encode(traintext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        trainB = infersent2.encode(traintext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "        print ('Computing ' + modelname + 'embedding for test data...')\n",
        "        \n",
        "        testA = infersent2.encode(testtext[0], bsize=128, tokenize=False, verbose=True)\n",
        "        testB = infersent2.encode(testtext[1], bsize=128, tokenize=False, verbose=True)\n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "    if modelname ==\"use_v3\":\n",
        "        \n",
        "        print ('Computing training USE encoding...')\n",
        "        with tf.Session() as session:\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "            trainA = session.run(embed(traintext[0]))\n",
        "            trainB = session.run(embed(traintext[1]))\n",
        "            testA = session.run(embed(testtext[0]))\n",
        "            testB = session.run(embed(testtext[1]))\n",
        "    \n",
        "    train_features = np.c_[np.abs(trainA - trainB), trainA * trainB]\n",
        "    test_features = np.c_[np.abs(testA - testB), testA * testB]\n",
        "        \n",
        "    \n",
        "    return train_features, test_features\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Ukjst-Oj8_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(loc='./dataset/'):\n",
        "    \"\"\"\n",
        "    Load test and train dataset\n",
        "    \"\"\"\n",
        "    trainloc = os.path.join(loc, 'Competition_Train_Data.txt')\n",
        "    testloc = os.path.join(loc, 'Competition_Test_Data.txt')\n",
        "    \n",
        "    print(trainloc)\n",
        "\n",
        "    trainA, trainB, testA, testB = [],[],[],[]\n",
        "    trainS, testU, testS = [],[],[]\n",
        "\n",
        "    f = open(trainloc, 'r')\n",
        "    for line in f:\n",
        "        text = line.strip().split('\\t')\n",
        "        trainA.append(' '.join(word_tokenize(text[2])))\n",
        "        trainB.append(' '.join(word_tokenize(text[3])))\n",
        "        trainS.append(text[0])\n",
        "    f.close()\n",
        "    f = open(testloc, 'r')\n",
        "    for line in f:\n",
        "        text = line.strip().split('\\t')\n",
        "        testA.append(' '.join(word_tokenize(text[1])))\n",
        "        testB.append(' '.join(word_tokenize(text[2])))\n",
        "        #testS.append(text[3])\n",
        "        testU.append(text[0])\n",
        "    f.close()\n",
        "    \n",
        "   \n",
        "    trainS = [int(s) for s in trainS[1:]]\n",
        "    testU = [int(s) for s in testU[1:]]\n",
        "\n",
        "    return [trainA[1:], trainB[1:]], [testA[1:], testB[1:]], [trainS, testS], testU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAhy_ifemh7W",
        "colab_type": "code",
        "outputId": "ec715c17-f30d-4629-bccf-896a2858de04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "traintext, testtext, labels, uid = load_data()\n",
        "fts_train_1, fts_test_1 = generate_sentense_fts(modelname = 'use_v3')\n",
        "fts_train_2, fts_test_2 = generate_sentense_fts(modelname = 'infersent')\n",
        "fts_train_3, fts_test_3 = generate_sentense_fts(modelname = 'infersent2')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./dataset/Competition_Train_Data.txt\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing training USE encoding...\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing infersentembedding for train data...\n",
            "Nb words kept : 110074/111322 (98.9%)\n",
            "Speed : 929.6 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 109905/111146 (98.9%)\n",
            "Speed : 968.6 sentences/s (gpu mode, bsize=128)\n",
            "Computing infersentembedding for test data...\n",
            "Nb words kept : 27576/27876 (98.9%)\n",
            "Speed : 903.4 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 27606/27912 (98.9%)\n",
            "Speed : 922.0 sentences/s (gpu mode, bsize=128)\n",
            "Preparing data...\n",
            "./dataset/Competition_Train_Data.txt\n",
            "Computing infersent2embedding for train data...\n",
            "Nb words kept : 107610/111322 (96.7%)\n",
            "Speed : 898.3 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 107320/111146 (96.6%)\n",
            "Speed : 906.8 sentences/s (gpu mode, bsize=128)\n",
            "Computing infersent2embedding for test data...\n",
            "Nb words kept : 26846/27876 (96.3%)\n",
            "Speed : 836.8 sentences/s (gpu mode, bsize=128)\n",
            "Nb words kept : 26917/27912 (96.4%)\n",
            "Speed : 858.2 sentences/s (gpu mode, bsize=128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "406qvxVzajdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "51ec0e1f-ec22-4730-d7e5-bcaed55a0a4a"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainfts, testfts = generate_spacy_cc_similarity()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "CPU times: user 5min 45s, sys: 2min, total: 7min 45s\n",
            "Wall time: 3min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKkQgtFt-lS7",
        "colab_type": "code",
        "outputId": "f6edf182-6a0e-4625-893b-430a22863ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_1.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "v1AlIW51tAta",
        "colab_type": "code",
        "outputId": "33dd9a89-1e95-4632-87b8-53255c00a159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_2.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 8192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "y8lCSS8VtExL",
        "colab_type": "code",
        "outputId": "56ca57a7-29d3-4f21-ab85-f8fe79405f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "fts_train_3.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 8192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "LPFGWgkCFqAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine all the features into a gigantic feature matrix"
      ]
    },
    {
      "metadata": {
        "id": "lBw9e52x3EHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_features =np.c_[fts_train_1,fts_train_2,fts_train_3,feats(traintext[0], traintext[1]),trainfts]\n",
        "test_features = np.c_[fts_test_1,fts_test_2,fts_test_3,feats(testtext[0], testtext[1]),testfts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qIohZKLlohH5",
        "colab_type": "code",
        "outputId": "6c19df7b-dac4-48b5-dca8-592b3b47399d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4640, 17415)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "vAtJX40QGEUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticate Colab to use google drive to store the pickle file"
      ]
    },
    {
      "metadata": {
        "id": "Zq4gvl7bI67W",
        "colab_type": "code",
        "outputId": "135397fb-4927-4c56-e49e-8a4b50663971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "371i6I0hK8sH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clO8nQpyGhmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the features and labels into pickle file"
      ]
    },
    {
      "metadata": {
        "id": "98k_D2TMdnYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = open('final_features.pkl','wb')\n",
        "\n",
        "pickle.dump(train_features, file)\n",
        "pickle.dump(test_features, file)\n",
        "pickle.dump(labels[0], file)\n",
        "pickle.dump(labels[1], file)\n",
        "pickle.dump(uid, file)\n",
        "\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NWoFDAejeBwR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Upload the feature matrix to Google Drive for modelling ~800 MB**"
      ]
    },
    {
      "metadata": {
        "id": "ymbJNG5hMBGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "upload = drive.CreateFile({'title': 'final_features.pkl'})\n",
        "upload.SetContentFile('final_features.pkl')\n",
        "upload.Upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxxSKDq_fMlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Go to Google Drive, select the pkl file Click Get shareable link.\n",
        "Next to \"Anyone with the link,\" click the Down arrow Down.\n",
        "Click More and then On - Public on the web.\n",
        "Click Save.\n",
        "Click Done.\n"
      ]
    },
    {
      "metadata": {
        "id": "Pl_0WohzIFpI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# https://drive.google.com/open?id=1xGT1h3XgWEmZTEkDPMpKIyK7odRNCqy6\n",
        "# https://drive.google.com/file/d/1xGT1h3XgWEmZTEkDPMpKIyK7odRNCqy6/view?usp=sharing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLe1T9j3Nr_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to download again\n",
        "#download = drive.CreateFile({'id': '1PVGVj-BHu5I3ANnEfG4_JKn6RgyzeW3d'})\n",
        "#download.GetContentFile('features.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1AAF9qCFIbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7261e4a0-b25d-4b3c-94ed-a0a43337d52c"
      },
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2277412\n",
            "-rw-r--r-- 1 root root 1523785255 May 24 11:52 crawl-300d-2M.vec.zip\n",
            "drwxr-xr-x 2 root root       4096 Nov  5 19:58 sample_data\n",
            "-rw-r--r-- 1 root root      29737 Nov  6 18:21 models.py\n",
            "drwxr-xr-x 2 root root       4096 Nov  6 18:22 encoder\n",
            "drwxr-xr-x 2 root root       4096 Nov  6 18:30 __pycache__\n",
            "drwxr-xr-x 4 root root       4096 Nov  6 18:31 dataset\n",
            "-rw-r--r-- 1 root root       2551 Nov  6 18:48 adc.json\n",
            "drwx------ 3 root root       4096 Nov  6 18:49 gdrive\n",
            "-rw-r--r-- 1 root root  808210640 Nov  6 19:20 final_features.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WBPAWuNFgUzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}